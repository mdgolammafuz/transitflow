"""
Feature Sync Job - Delta Lake to PostgreSQL.

Pattern: Identity-Mirroring Atomic Upsert
Methodical: Aligned with Phase 5 validated dbt Marts (marts.fct_stop_arrivals).
Robust: Uses dbt-generated feature_id to prevent duplicate explosion in Postgres.
"""

import logging
import os
import sys

import psycopg2
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def get_required_env(key: str) -> str:
    value = os.environ.get(key)
    if not value:
        raise EnvironmentError(f"CRITICAL: Environment variable {key} is not set.")
    return value


def get_spark_session() -> SparkSession:
    minio_endpoint = os.environ.get("MINIO_ENDPOINT", "http://minio:9000")
    minio_user = get_required_env("MINIO_ROOT_USER")
    minio_password = get_required_env("MINIO_ROOT_PASSWORD")

    return (
        SparkSession.builder.appName("TransitFlow-FeatureSync")
        .config(
            "spark.jars.packages",
            "io.delta:delta-spark_2.12:3.0.0,org.postgresql:postgresql:42.6.0",
        )
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config(
            "spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog"
        )
        .config("spark.hadoop.fs.s3a.endpoint", minio_endpoint)
        .config("spark.hadoop.fs.s3a.access.key", minio_user)
        .config("spark.hadoop.fs.s3a.secret.key", minio_password)
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .getOrCreate()
    )


def read_stop_performance(spark: SparkSession, bucket: str, target_date: str) -> DataFrame:
    """Read from Delta Lake Gold and align with dbt-generated feature_id."""
    gold_path = f"s3a://{bucket}/gold/stop_performance"
    logger.info(f"Loading Gold features from {gold_path}")

    # Read the Delta table and filter by the target date
    df = spark.read.format("delta").load(gold_path).filter(F.col("event_date") == target_date)

    # SELECT Logic:
    # 1. We pull 'feature_id' directly from Delta (generated by dbt) to ensure Upsert collision.
    return df.select(
        F.col("feature_id"),  # CRITICAL: Use dbt's identity anchor
        F.coalesce(F.trim(F.col("stop_id").cast("string")), F.lit("UNKNOWN")).alias("stop_id"),
        F.coalesce(F.trim(F.col("line_id").cast("string")), F.lit("UNKNOWN")).alias("line_id"),
        F.col("event_date"),
        F.col("hour_of_day").cast("int"),
        F.col("day_of_week").cast("int"),
        F.col("stop_lat").cast("double"),
        F.col("stop_lon").cast("double"),
        F.col("historical_avg_delay").cast("double"),
        F.col("historical_arrival_count").cast("long"),
        F.col("avg_dwell_time_ms").cast("double"),
        F.col("historical_stddev_delay").cast("double"),
    ).na.fill(0.0)


def write_to_postgres(df: DataFrame) -> int:
    """Execute Identity-Mirroring Upsert into marts.fct_stop_arrivals."""
    host = os.environ.get("POSTGRES_HOST", "postgres")
    port = os.environ.get("POSTGRES_PORT", "5432")
    db = os.environ.get("POSTGRES_DB", "transit")
    user = get_required_env("POSTGRES_USER")
    password = get_required_env("POSTGRES_PASSWORD")

    jdbc_url = f"jdbc:postgresql://{host}:{port}/{db}"
    properties = {"user": user, "password": password, "driver": "org.postgresql.Driver"}

    # 1. Write to temporary staging table
    temp_table = "marts.fct_stop_arrivals_staging"
    logger.info(f"Writing to staging table: {temp_table}")
    df.write.jdbc(url=jdbc_url, table=temp_table, mode="overwrite", properties=properties)

    # 2. Perform Atomic Upsert. 
    # Logic: Collision on 'feature_id' triggers an update, preventing the duplicate explosion.
    upsert_sql = """
        INSERT INTO marts.fct_stop_arrivals (
            feature_id, stop_id, line_id, event_date, hour_of_day, day_of_week,
            stop_lat, stop_lon,
            historical_arrival_count, historical_avg_delay,
            historical_stddev_delay, avg_dwell_time_ms
        )
        SELECT
            feature_id, stop_id, line_id, event_date, hour_of_day, day_of_week,
            stop_lat, stop_lon,
            historical_arrival_count, historical_avg_delay,
            historical_stddev_delay, avg_dwell_time_ms
        FROM marts.fct_stop_arrivals_staging
        ON CONFLICT (feature_id)
        DO UPDATE SET
            historical_arrival_count = EXCLUDED.historical_arrival_count,
            historical_avg_delay = EXCLUDED.historical_avg_delay,
            historical_stddev_delay = EXCLUDED.historical_stddev_delay,
            avg_dwell_time_ms = EXCLUDED.avg_dwell_time_ms,
            stop_lat = EXCLUDED.stop_lat,
            stop_lon = EXCLUDED.stop_lon;
    """

    conn = psycopg2.connect(host=host, port=port, user=user, password=password, dbname=db)
    try:
        with conn.cursor() as cur:
            cur.execute(upsert_sql)
            cur.execute(f"DROP TABLE IF EXISTS {temp_table}")
        conn.commit()
        logger.info("Identity-Mirroring upsert completed successfully.")
    finally:
        conn.close()

    return df.count()


def run_feature_sync(target_date: str, bucket: str = "transitflow-lakehouse") -> dict:
    spark = get_spark_session()
    try:
        stop_features = read_stop_performance(spark, bucket, target_date)
        
        count = stop_features.count()
        if count == 0:
            logger.warning(f"No features found for date {target_date}. Aborting.")
            return {"status": "no_data", "rows_written": 0}

        rows_written = write_to_postgres(stop_features)
        return {"status": "success", "rows_written": rows_written}
    except Exception as e:
        logger.error(f"Feature sync failed: {e}")
        return {"status": "failed", "error": str(e)}
    finally:
        spark.stop()


if __name__ == "__main__":
    target_date = None
    if len(sys.argv) > 1:
        target_date = sys.argv[1]
    else:
        target_date = os.environ.get("DATE")

    if not target_date:
        logger.error("CRITICAL: No target date provided.")
        sys.exit(1)

    logger.info(f"Starting Feature Sync for date: {target_date}")
    run_feature_sync(target_date=target_date)